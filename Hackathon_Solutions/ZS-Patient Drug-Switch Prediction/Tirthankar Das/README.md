The objective of Patient Drug Switch Hackathon was to identify the patient population who are likely to switch any product in RA (Rheumatoid Arthritis therapeutic) Market. It had three layers to evaluate the solution. First round was evaluating the best solution based on AUC. Second round was based on ‘Time and Memory Complexity’. This second step is really relevant in the practical scenario from a deployment perspective. Most of the Hackathon ignores this point. Other than ‘MacBook’, this second step really made this Hackathon interesting. And the final round was presented in the room which was full of data science enthusiasts. 

In the Hackathon, we were given transaction data of the patient for both train and test. We had to create features from that transaction data of drug purchasing. It had six columns: Patient_id, Time, Event, Specialty, Plan_Type and Payment. Event, Specialty and Plan_type are different sub levels to represent the drug. I always try to follow the traditional approach of model building which has three building blocks. Please, find the details in the following:

A) Feature Generation:
In the Hackathon, we were given a brief idea about how to develop features. There were three broad types of features i.e. Recency, Frequency and NormChange. Recency is defined as how recently did an Event/Plan_Type/Specialty happen prior to the anchor date, Frequency means how many times did an Event/Plan_Type/Specialty happen in a specific time frame and NormChange means whether the frequency of an Event/Plan_Type/Specialty increased or decreased in a recent time frame (not more than 1.5 years) as compared to the previous time frame. Around 40k variables were generated with the above-mentioned logic. As I mentioned earlier, this Hackathon is not only about the accuracy of the model. It is also about tackling time and memory complexity. Creating 40k features sequentially will take humongous time if we run it on the machine with RAM of 16GB with 4 cores (as per given specification). To reduce the time consumption, I introduced parallel processing concept for calculating features of Frequency and NormChange whereas I used default Apply function of python for calculating Recency features. 

B) Feature Selection:
Once all the features were created, I did two preprocessing before feeding these into the LightGBM model. Imputing the missing value with 9999999 and removing degenerate variables. It helped to reduce the number of features. After this step, remaining features were fed into the LightGBM model. I did not do much tuning in this step as it was just a feature selection step. I used feature importance to select the final set of features. The logic of subsetting the feature was very straight forward i.e. selecting all the features whose importance is greater than zero. Finally, I had 7k features for the final model. Recency features were found most important compared to NormChange and Frequency.

C) Model building:
LightGBM is always my favourite algorithm whenever memory and time are constrained. In the final model, I have used stratified k fold cross-validation where the value of K is 30. Final prediction came from the average of those models. Tuning of hyper parameters such as learning rate, feature fraction and num_leaves were important which impacted the result.
